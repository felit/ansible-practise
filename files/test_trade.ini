[KafkaConsumer]
# source database
src_db =host=192.168.181.41 port=5432 user=pgq password=pgq dbname=trade
# query to call
## Deprecated, use table_filter ##
# filter for events (SQL fragment)
table_filter=trade.merchant,trade.merchant_info,trade.card,trade.trans_water,trade.trans_water_his,trade.trans_water_his_babeila,trade.trans_water_his_heji,trade.trans_water_his_jbw,trade.trans_water_his_meizhou,trade.trans_water_his_other,trade.trans_water_his_quanjincheng,trade.trans_water_his_tmp,trade.trans_water_his_xld,trade.trans_water_his_yichayizuo,trade.coupon,trade.cardtype_grade,trade.membership,trade.coupon_his,trade.coupon_type,trade.membership_birthday_family,trade.evaluate
## Parameters for pgq.Consumer ##
# queue name to read from
queue_name = test_trade
subconsumer_name = %(job_name)sub1
subconsumer_timeout = 600

# override consumer name
consumer_name = test_consumer

#table_filter = public.t1

# whether to use cursor to fetch events (0 disables)
#pgq_lazy_fetch = 1

# whether to read from source size in autocommmit mode
# not compatible with pgq_lazy_fetch
# the actual user script on top of pgq.Consumer must also support it
pgq_autocommit = 0

# whether to wait for specified number of events,
# before assigning a batch (0 disables)
#pgq_batch_collect_events = 0

# whether to wait specified amount of time,
# before assigning a batch (postgres interval)
#pgq_batch_collect_interval =

# whether to stay behind queue top (postgres interval)
#pgq_keep_lag =

# in how many seconds to write keepalive stats for idle consumers
# this stats is used for detecting that consumer is still running
keepalive_stats = 30

## Parameters for skytools.DBScript ##

# default lifetime for database connections (in seconds)
#connection_lifetime = 1200

## Parameters for skytools.BaseScript ##

# how many seconds to sleep between work loops
# if missing or 0, then instead sleeping, the script will exit
loop_delay = 1.0

# where to log
logfile = %(job_name)s.log

# where to write pidfile
pidfile = %(job_name)s.pid

# per-process name to use in logging
#job_name = %(config_name)s

# whether centralized logging should be used
# search-path [ ./skylog.ini, ~/.skylog.ini, /etc/skylog.ini ]
#   0 - disabled
#   1 - enabled, unless non-daemon on console (os.isatty())
#   2 - always enabled
#use_skylog = 0

# where to find skylog.ini
#skylog_locations = skylog.ini, ~/.skylog.ini, /etc/skylog.ini

# how many seconds to sleep after catching a exception
#exception_sleep = 20
kafka_server = 192.168.181.24:9092
#kafka_server =192.168.181.21:9092,192.168.181.22:9092,192.168.181.23:9092 
mongodb_hosts = 192.168.236.62:50000
mongodb_db = storm_test_params
mongodb_enable=true
